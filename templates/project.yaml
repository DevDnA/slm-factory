# slm-factory 프로젝트 설정 파일
# 전체 파이프라인: parse → generate QA → validate → convert → train → export
# 이 파일은 slm-factory의 파이프라인 설정입니다. 수정 없이도 기본값으로 wizard를 실행할 수 있습니다.

project:
  name: "my-project"
  version: "1.0.0"
  language: "en"                        # "en", "ko", "ja" 등 문서 언어 코드

paths:
  documents: "./documents"              # 입력 문서 디렉토리
  output: "./output"                    # 모든 출력물이 저장되는 디렉토리

# -- 1단계: 문서 파싱(Document Parsing) ------------------------------------
parsing:
  formats: ["pdf", "txt", "html"]        # pdf, hwpx, html, txt, md, docx 지원
  pdf:
    extract_tables: true
  hwpx:
    apply_spacing: true                 # 필수 설치: pip install slm-factory[korean]

# -- 2단계: QA 생성(QA Generation) -----------------------------------------
teacher:
  backend: "ollama"                     # "ollama" (로컬) | "openai" (API/vLLM). vLLM은 openai 백엔드 사용
                                        # ⚠ ollama 사용 시 사전 실행 필요: ollama serve
  model: "qwen3:8b"                     # ⚠ 사전 다운로드 필요: ollama pull qwen3:8b
                                        # 추천: qwen3:8b (다국어), llama3.1:8b (영어), gemma2:9b (품질)
  api_base: "http://localhost:11434"    # ollama 기본 주소. vLLM: "http://localhost:8000/v1"
  api_key: null                         # ollama는 불필요. openai 백엔드 사용 시 필수
  temperature: 0.3                      # 낮을수록 일관된 답변 (0.1~0.5 권장)
  timeout: 180                          # 초 단위. 긴 문서 처리 시 300~600으로 증가
  max_context_chars: 12000              # Teacher 모델에 한 번에 보내는 문서 글자 수. 문서가 길면 이 단위로 나눠서 처리
  max_concurrency: 4                    # 동시 요청 수. Ollama: 1~2, vLLM: 8~16 권장

questions:
  categories:
    overview:
      - "What is the main purpose of this document?"
      - "What problem does this aim to solve?"
      - "Who are the target users or beneficiaries?"
      - "What are the key changes or innovations described?"
      - "What is the scope and limitations?"
    technical:
      - "What are the key technical details or specifications?"
      - "What technologies, methods, or tools are described?"
      - "What are the system requirements?"
      - "How does this compare to existing alternatives?"
      - "What are the performance metrics or benchmarks?"
    implementation:
      - "What is the timeline or schedule?"
      - "What resources (budget, personnel, infrastructure) are required?"
      - "What are the step-by-step implementation procedures?"
      - "What risks or challenges are anticipated?"
      - "What are the expected outcomes or deliverables?"
    # 파일에서 질문을 불러오려면 아래 주석 해제:
    # file: "./questions.txt"

  system_prompt: >
    You are a helpful assistant that answers questions based strictly on the
    provided document. Answer only from the document content. Do not speculate
    or fabricate information. Be concise and factual. Include specific numbers,
    dates, and names when available. If the document does not contain relevant
    information, say "The document does not contain this information."

  output_format: "alpaca"

# -- 3단계: QA 검증(QA Validation) -----------------------------------------
validation:
  enabled: true
  min_answer_length: 20
  max_answer_length: 2000
  remove_empty: true
  deduplicate: true
  reject_patterns:
    - "(?i)i don't know"
    - "(?i)not (available|provided|mentioned|found)"
    - "(?i)the document does not contain"
  groundedness:
    enabled: false                      # 필수 설치: pip install slm-factory[validation]
    model: "all-MiniLM-L6-v2"
    threshold: 0.3

# -- 3a단계: 품질 점수 평가(Quality Scoring) --------------------------------
scoring:
  enabled: false                        # Teacher LLM이 QA 쌍을 1~5점으로 평가합니다
                                        # 활성화 시 LLM 호출 추가 → 시간 2배 증가
  threshold: 3.0                        # 이 점수 미만의 QA 쌍을 제거합니다 (1.0~5.0)
  max_concurrency: 4                    # 동시 평가 요청 수

# -- 3b단계: 데이터 증강(Data Augmentation) ---------------------------------
augment:
  enabled: false                        # 질문을 다양한 표현으로 변형하여 학습 데이터를 늘립니다
                                        # QA 100개 미만일 때 활성화 권장
  num_variants: 2                       # 각 질문당 생성할 변형 수 (데이터가 N배 증가)
  max_concurrency: 4                    # 동시 증강 요청 수

# -- 3c단계: 데이터 분석(Data Analysis) ------------------------------------
analyzer:
  enabled: true                       # QA 쌍 통계 보고서를 생성합니다
  output_file: "data_analysis.json"   # 분석 보고서 저장 파일명

# -- 4·5단계: 학생 모델(Student Model) + 학습(Training) --------------------
student:
  model: "google/gemma-3-1b-it"         # ⚠ 첫 실행 시 HuggingFace에서 자동 다운로드 (~2GB)
                                        # 추천: gemma-3-1b-it (경량), Qwen/Qwen3-1.7B (다국어)
  max_seq_length: 4096                  # 이 토큰 수를 초과하는 QA 쌍은 convert 단계에서 제외됩니다
                                        # → 줄이면 긴 QA가 버려지지만 VRAM 사용량 감소. 부족 시 2048

training:
  # --- LoRA 설정 ---
  # LoRA: 모델 전체를 학습하지 않고 일부분만 학습하여 VRAM을 절약하는 기법
  lora:
    r: 16                               # 높이면 → 더 많이 학습하여 품질↑, 대신 VRAM↑ 학습시간↑
                                        # 낮추면 → 가볍고 빠르지만 학습 품질 제한 (8, 16, 32, 64)
    alpha: 32                            # 학습 내용이 모델에 반영되는 강도. 보통 r의 2배로 설정
                                        # → 높이면 도메인 답변 비중↑, 낮추면 원래 모델의 답변 스타일 유지
    dropout: 0.05                        # 학습 데이터에만 맞춰지는 현상(과적합)을 방지 (0.0~0.1)
                                        # → QA가 적을 때(<100개) 0.1로 올리면 새 질문에도 잘 대답
    target_modules: "auto"               # 모델에서 학습할 부분을 자동 선택. 대부분 "auto"로 충분
                                        # → 고급 사용자만 직접 지정: ["q_proj", "v_proj"]
    use_rslora: false                    # r을 32 이상으로 크게 설정할 때 학습이 불안정해지는 것을 방지
                                        # → r이 16 이하면 false로 충분

  batch_size: 4                         # 한 번에 학습하는 QA 샘플 수. VRAM 사용량을 가장 크게 좌우
                                        # → VRAM별 권장값: 8GB→2, 16GB→4~8, 24GB→8~16
  gradient_accumulation_steps: 4         # 이 횟수만큼 모아서 한 번에 학습. 실질 배치 = 4 × 4 = 16
                                        # → VRAM 부족으로 batch_size를 줄였을 때, 이 값을 올리면 보완됨
  learning_rate: 2.0e-5                 # 한 번에 얼마나 많이 배울지. 클수록 빠르지만 불안정해질 수 있음
                                        # → QA 데이터가 적으면(<100개) 1e-5로 낮추는 것을 권장
  lr_scheduler: "cosine"                # 학습이 진행될수록 learning_rate를 어떻게 줄일지 결정
                                        # → "cosine": 후반부에 서서히 감소 (권장)
                                        # → "linear": 일정하게 감소 | "constant": 줄이지 않음
  warmup_ratio: 0.1                     # 학습 초반에 learning_rate를 천천히 올리는 구간 (전체의 10%)
                                        # → 초반 학습이 불안정할 때 이 값을 0.15~0.2로 올려보세요
  num_epochs: 20                        # 전체 QA 데이터를 최대 몇 바퀴 반복 학습할지 (상한선)
                                        # → early_stopping이 켜져 있으면 이 수 전에 자동 종료됩니다
                                        # → QA 100개 이하: 10~20, 1000개 이상: 3~5 권장

  early_stopping:
    enabled: true                        # 학습 효과가 없으면 num_epochs 전에 자동으로 멈춥니다
    patience: 3                          # 연속 3회 평가에서 나아지지 않으면 학습 중단
    threshold: 0.01                      # 손실(loss)이 0.01 이상 줄어야 "나아졌다"고 판단
                                        # → 줄이면(0.005) 미세한 개선도 인정 → 더 오래 학습
                                        # → 올리면(0.05) 큰 개선만 인정 → 더 빨리 종료

  optimizer: "adamw_torch_fused"         # GPU 최적화 버전. CPU 학습 시 "adamw_torch"로 변경
  bf16: true                             # 메모리 절반 사용 + 학습 속도 향상. RTX 30xx/A100 이상 지원
                                        # → GPU가 미지원 시 false로 변경 (오류 메시지로 확인 가능)
  train_split: 0.9                       # QA 데이터 중 90%는 학습용, 10%는 성능 측정용으로 분리
                                        # → 데이터가 적으면(<50개) 0.95로 올려 학습 데이터를 확보
  save_strategy: "epoch"                 # 매 에포크 끝에 체크포인트 저장 + 성능 측정 실행
                                        # → early_stopping 판단도 이 주기마다 실행됩니다
                                        # → "steps": 에포크 대신 일정 스텝마다 실행

  quantization:
    enabled: false                       # true로 바꾸면 모델을 4비트로 압축 로드하여 VRAM 크게 절약
                                        # → VRAM 8GB 이하에서 학습해야 할 때 활성화 (속도는 다소 감소)
    bits: 4                              # 양자화 비트 수. 4가 기본 (4 | 8)

# -- 자동 평가(Evaluation) -------------------------------------------------
# eval:
#   enabled: false                      # 학습된 모델을 held-out QA로 자동 평가합니다
#   test_split: 0.1                     # 전체 QA 중 평가용 비율 (0.0~1.0)
#   metrics: ["bleu", "rouge"]          # 평가 메트릭: bleu, rouge, bertscore
#   max_samples: 50                     # 최대 평가 샘플 수
#   output_file: "eval_results.json"    # 평가 결과 저장 파일

# -- GGUF 변환(GGUF Export) ------------------------------------------------
# gguf_export:
#   enabled: false                      # llama.cpp GGUF 포맷으로 변환합니다
#   quantization_type: "q4_k_m"         # q4_k_m (권장), q5_k_m, q8_0, f16
#   llama_cpp_path: ""                  # llama.cpp 디렉토리 경로 (빈 값이면 자동 탐색)

# -- 증분 학습(Incremental Learning) ----------------------------------------
# incremental:
#   enabled: false                      # 문서 변경분만 QA를 재생성합니다
#   hash_file: "document_hashes.json"   # 문서 해시 저장 파일
#   merge_strategy: "append"            # "append" (기존 유지+추가) | "replace" (전체 교체)
#   resume_adapter: ""                  # 이전 어댑터 경로 (이어서 학습)

# -- 멀티턴 대화 생성(Dialogue Generation) -----------------------------------
# dialogue:
#   enabled: false                      # 단일 QA를 멀티턴 대화로 확장합니다
#   min_turns: 2                        # 최소 대화 턴 수
#   max_turns: 5                        # 최대 대화 턴 수
#   include_single_qa: true             # 단일 QA도 학습 데이터에 포함

# -- QA 리뷰(Review) -------------------------------------------------------
# review:
#   enabled: false                      # TUI에서 QA 쌍을 수동 검토합니다
#   auto_open: true                     # review 명령 시 자동으로 TUI 실행
#   output_file: "qa_reviewed.json"     # 리뷰 결과 저장 파일

# -- 모델 비교(Compare) ----------------------------------------------------
# compare:
#   enabled: false                      # 기본 모델 vs 파인튜닝 모델 답변 비교
#   base_model: ""                      # 비교 기준 모델 (Ollama 모델명)
#   finetuned_model: ""                 # 파인튜닝된 모델 (Ollama 모델명)
#   metrics: ["bleu", "rouge"]          # 비교에 사용할 메트릭
#   max_samples: 20                     # 비교할 최대 질문 수
#   output_file: "compare_results.json" # 비교 결과 저장 파일

# -- TUI 대시보드(Dashboard) ------------------------------------------------
# dashboard:
#   enabled: false                      # 터미널 대시보드로 파이프라인 모니터링
#   refresh_interval: 2.0               # 화면 갱신 주기 (초)
#   theme: "dark"                       # "dark" | "light"

# -- 6단계: 모델 내보내기(Export) -------------------------------------------
export:
  merge_lora: true                      # 학습 결과를 원래 모델에 합쳐서 하나의 완성 모델로 저장
  output_format: "safetensors"          # "safetensors" (권장) | "pytorch"
  ollama:
    enabled: true                       # Ollama 모델로 내보내기 (ollama create 실행)
    model_name: "my-project-model"      # `ollama run my-project-model`로 실행할 모델명
                                        # 소문자, 하이픈 사용 권장
    system_prompt: "You are a helpful domain-specific assistant."
                                        # 모델의 기본 시스템 프롬프트
    parameters:
      temperature: 0.7                  # 추론 시 온도. 높을수록 다양한 답변
      top_p: 0.9                        # 답변 다양성 조절. 낮추면(0.5) 안전한 답변, 높이면(0.95) 다양한 표현
      num_ctx: 4096                     # 모델이 한 번에 읽을 수 있는 글자 수 (질문 + 답변 포함)

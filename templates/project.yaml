# slm-factory 프로젝트 설정 파일
# 전체 파이프라인: parse → generate QA → validate → convert → train → export
# 이 파일은 slm-factory의 파이프라인 설정입니다. 수정 없이도 기본값으로 wizard를 실행할 수 있습니다.

project:
  name: "my-project"
  version: "1.0.0"
  language: "en"                        # "en", "ko", "ja" 등 문서 언어 코드

paths:
  documents: "./documents"              # 입력 문서 디렉토리
  output: "./output"                    # 모든 출력물이 저장되는 디렉토리

# -- 1단계: 문서 파싱(Document Parsing) ------------------------------------
parsing:
  formats: ["pdf", "txt", "html"]        # pdf, hwpx, html, txt, md, docx 지원
  pdf:
    extract_tables: true
  hwpx:
    apply_spacing: true                 # 필수 설치: pip install slm-factory[korean]

# -- 2단계: QA 생성(QA Generation) -----------------------------------------
teacher:
  backend: "ollama"                     # "ollama" (로컬) | "openai" (API/vLLM). vLLM은 openai 백엔드 사용
                                        # ⚠ ollama 사용 시 사전 실행 필요: ollama serve
  model: "qwen3:8b"                     # ⚠ 사전 다운로드 필요: ollama pull qwen3:8b
                                        # 추천: qwen3:8b (다국어), llama3.1:8b (영어), gemma2:9b (품질)
  api_base: "http://localhost:11434"    # ollama 기본 주소. vLLM: "http://localhost:8000/v1"
  api_key: null                         # ollama는 불필요. openai 백엔드 사용 시 필수
  temperature: 0.3                      # 낮을수록 일관된 답변 (0.1~0.5 권장)
  timeout: 180                          # 초 단위. 긴 문서 처리 시 300~600으로 증가
  max_context_chars: 12000              # 문서 청크 최대 크기. 모델 컨텍스트에 맞게 조절
  max_concurrency: 4                    # 동시 요청 수. Ollama: 1~2, vLLM: 8~16 권장

questions:
  categories:
    overview:
      - "What is the main purpose of this document?"
      - "What problem does this aim to solve?"
      - "Who are the target users or beneficiaries?"
      - "What are the key changes or innovations described?"
      - "What is the scope and limitations?"
    technical:
      - "What are the key technical details or specifications?"
      - "What technologies, methods, or tools are described?"
      - "What are the system requirements?"
      - "How does this compare to existing alternatives?"
      - "What are the performance metrics or benchmarks?"
    implementation:
      - "What is the timeline or schedule?"
      - "What resources (budget, personnel, infrastructure) are required?"
      - "What are the step-by-step implementation procedures?"
      - "What risks or challenges are anticipated?"
      - "What are the expected outcomes or deliverables?"
    # 파일에서 질문을 불러오려면 아래 주석 해제:
    # file: "./questions.txt"

  system_prompt: >
    You are a helpful assistant that answers questions based strictly on the
    provided document. Answer only from the document content. Do not speculate
    or fabricate information. Be concise and factual. Include specific numbers,
    dates, and names when available. If the document does not contain relevant
    information, say "The document does not contain this information."

  output_format: "alpaca"

# -- 3단계: QA 검증(QA Validation) -----------------------------------------
validation:
  enabled: true
  min_answer_length: 20
  max_answer_length: 2000
  remove_empty: true
  deduplicate: true
  reject_patterns:
    - "(?i)i don't know"
    - "(?i)not (available|provided|mentioned|found)"
    - "(?i)the document does not contain"
  groundedness:
    enabled: false                      # 필수 설치: pip install slm-factory[validation]
    model: "all-MiniLM-L6-v2"
    threshold: 0.3

# -- 3a단계: 품질 점수 평가(Quality Scoring) --------------------------------
scoring:
  enabled: false                        # Teacher LLM이 QA 쌍을 1~5점으로 평가합니다
                                        # 활성화 시 LLM 호출 추가 → 시간 2배 증가
  threshold: 3.0                        # 이 점수 미만의 QA 쌍을 제거합니다 (1.0~5.0)
  max_concurrency: 4                    # 동시 평가 요청 수

# -- 3b단계: 데이터 증강(Data Augmentation) ---------------------------------
augment:
  enabled: false                        # 질문을 다양한 표현으로 변형하여 학습 데이터를 늘립니다
                                        # QA 100개 미만일 때 활성화 권장
  num_variants: 2                       # 각 질문당 생성할 변형 수 (데이터가 N배 증가)
  max_concurrency: 4                    # 동시 증강 요청 수

# -- 3c단계: 데이터 분석(Data Analysis) ------------------------------------
analyzer:
  enabled: true                       # QA 쌍 통계 보고서를 생성합니다
  output_file: "data_analysis.json"   # 분석 보고서 저장 파일명

# -- 4·5단계: 학생 모델(Student Model) + 학습(Training) --------------------
student:
  model: "google/gemma-3-1b-it"         # ⚠ 첫 실행 시 HuggingFace에서 자동 다운로드 (~2GB)
                                        # 추천: gemma-3-1b-it (경량), Qwen/Qwen3-1.7B (다국어)
  max_seq_length: 4096                  # 학습 시 최대 시퀀스 길이. VRAM 부족 시 2048로 감소

training:
  # --- LoRA 설정 ---
  lora:
    r: 16                               # LoRA rank. 높을수록 표현력↑ 메모리↑ (8, 16, 32, 64)
    alpha: 32                            # 보통 r의 2배
    dropout: 0.05                        # 과적합 방지 (0.0~0.1)
    target_modules: "auto"               # "auto"는 모델에 맞게 자동 선택
    use_rslora: false                    # Rank-Stabilized LoRA. r이 큰 경우 활성화 권장

  batch_size: 4                         # GPU VRAM에 따라: 8GB→2, 16GB→4~8, 24GB→8~16
  gradient_accumulation_steps: 4         # 실질 배치 = batch_size × 이 값 (기본 16)
  learning_rate: 2.0e-5                 # 일반적으로 1e-5 ~ 5e-5. 데이터 적으면 낮게
  lr_scheduler: "cosine"                # "cosine" | "linear" | "constant"
  warmup_ratio: 0.1                     # 전체 스텝의 10%를 워밍업에 사용
  num_epochs: 20                        # 데이터 100개 이하: 10~20, 1000개 이상: 3~5 권장
                                        # early_stopping이 자동으로 조기 종료합니다

  early_stopping:
    enabled: true                        # 검증 손실이 개선되지 않으면 자동 종료
    patience: 3                          # N 에포크 동안 개선 없으면 종료
    threshold: 0.01                      # 이 값 이상 개선되어야 "개선"으로 인정

  optimizer: "adamw_torch_fused"         # GPU에서 가장 빠른 옵티마이저
  bf16: true                             # bfloat16 학습. GPU가 미지원 시 false로 변경
  train_split: 0.9                       # 90% 학습, 10% 검증
  save_strategy: "epoch"                 # "epoch" | "steps". 체크포인트 저장 주기

  quantization:
    enabled: false                       # QLoRA 활성화 (VRAM 절약, 속도 감소)
    bits: 4                              # 4-bit 양자화 (4 | 8)

# -- 자동 평가(Evaluation) -------------------------------------------------
# eval:
#   enabled: false                      # 학습된 모델을 held-out QA로 자동 평가합니다
#   test_split: 0.1                     # 전체 QA 중 평가용 비율 (0.0~1.0)
#   metrics: ["bleu", "rouge"]          # 평가 메트릭: bleu, rouge, bertscore
#   max_samples: 50                     # 최대 평가 샘플 수
#   output_file: "eval_results.json"    # 평가 결과 저장 파일

# -- GGUF 변환(GGUF Export) ------------------------------------------------
# gguf_export:
#   enabled: false                      # llama.cpp GGUF 포맷으로 변환합니다
#   quantization_type: "q4_k_m"         # q4_k_m (권장), q5_k_m, q8_0, f16
#   llama_cpp_path: ""                  # llama.cpp 디렉토리 경로 (빈 값이면 자동 탐색)

# -- 증분 학습(Incremental Learning) ----------------------------------------
# incremental:
#   enabled: false                      # 문서 변경분만 QA를 재생성합니다
#   hash_file: "document_hashes.json"   # 문서 해시 저장 파일
#   merge_strategy: "append"            # "append" (기존 유지+추가) | "replace" (전체 교체)
#   resume_adapter: ""                  # 이전 어댑터 경로 (이어서 학습)

# -- 멀티턴 대화 생성(Dialogue Generation) -----------------------------------
# dialogue:
#   enabled: false                      # 단일 QA를 멀티턴 대화로 확장합니다
#   min_turns: 2                        # 최소 대화 턴 수
#   max_turns: 5                        # 최대 대화 턴 수
#   include_single_qa: true             # 단일 QA도 학습 데이터에 포함

# -- QA 리뷰(Review) -------------------------------------------------------
# review:
#   enabled: false                      # TUI에서 QA 쌍을 수동 검토합니다
#   auto_open: true                     # review 명령 시 자동으로 TUI 실행
#   output_file: "qa_reviewed.json"     # 리뷰 결과 저장 파일

# -- 모델 비교(Compare) ----------------------------------------------------
# compare:
#   enabled: false                      # 기본 모델 vs 파인튜닝 모델 답변 비교
#   base_model: ""                      # 비교 기준 모델 (Ollama 모델명)
#   finetuned_model: ""                 # 파인튜닝된 모델 (Ollama 모델명)
#   max_samples: 20                     # 비교할 최대 질문 수
#   output_file: "compare_results.json" # 비교 결과 저장 파일

# -- TUI 대시보드(Dashboard) ------------------------------------------------
# dashboard:
#   enabled: false                      # 터미널 대시보드로 파이프라인 모니터링
#   refresh_interval: 2.0               # 화면 갱신 주기 (초)
#   theme: "dark"                       # "dark" | "light"

# -- 6단계: 모델 내보내기(Export) -------------------------------------------
export:
  merge_lora: true                      # LoRA 어댑터를 기본 모델에 병합
  output_format: "safetensors"          # "safetensors" (권장) | "pytorch"
  ollama:
    enabled: true                       # Ollama 모델로 내보내기 (ollama create 실행)
    model_name: "my-project-model"      # `ollama run my-project-model`로 실행할 모델명
                                        # 소문자, 하이픈 사용 권장
    system_prompt: "You are a helpful domain-specific assistant."
                                        # 모델의 기본 시스템 프롬프트
    parameters:
      temperature: 0.7                  # 추론 시 온도. 높을수록 다양한 답변
      top_p: 0.9                        # 누적 확률 기반 샘플링
      num_ctx: 4096                     # 추론 시 컨텍스트 길이
